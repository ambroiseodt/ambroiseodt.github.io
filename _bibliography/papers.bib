---
---

@article{odonnat2024leveraging,
  title={Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias},
  author={Odonnat, Ambroise and Feofanov, Vasilii and Redko, Ievgen},
  journal={International Conference on  Artificial Intelligence and Statistics (AISTATS)},
  year={2024},
  html={https://proceedings.mlr.press/v238/odonnat24a.html},
  pdf = 	 {https://proceedings.mlr.press/v238/odonnat24a/odonnat24a.pdf},
  abstract = 	 { Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, although they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraints. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on classification datasets of various data modalities. The code is available at https://github.com/ambroiseodt/tsim.},
  selected = {true},
  preview={intro_plot.pdf}
}

@article{ilbert2024unlocking,
      title={Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention}, 
      author={Ilbert*, Romain and Odonnat*, Ambroise and Feofanov, Vasilii and Virmaux, Aladin and Paolo, Giuseppe and Palpanas, Themis and Redko, Ievgen},
      journal={Preprint},
      year={2024},
      html={https://arxiv.org/abs/2402.10198},
      pdf={https://arxiv.org/pdf/2402.10198.pdf},
      abstract={Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times fewer parameters. The code is available at https://github.com/romilbert/samformer.},
      selected = {true},
      preview={samformer_architecture.PNG}
}

@article{xie2024leveraging,
      title={Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift}, 
      author={Xie, Renchunzi and Odonnat, Ambroise and Feofanov, Vasilii and Redko, Ievgen and Zhang, Jianfeng and An, Bo},
      journal={Preprint},
      year={2024},
      html={https://arxiv.org/abs/2401.08909},
      pdf={https://arxiv.org/pdf/2401.08909.pdf},
      abstract={Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach ensuring its empirical success. Extensive experiments conducted on diverse distribution shifts and model structures demonstrate that our method significantly outperforms state-of-the-art algorithms.},
      selected = {true},
      preview={gdscore.PNG}
}

@article{odonnat2022detection,
      title={Detection of interictal epileptiform discharges on EEG and MEG}, 
      author={Odonnat, Ambroise and Nasiotis, Konstantinos and Hill, Eleanor and Ebrahimi Kahou, Samira and Gnassounou, Theo and Zheng, Jiayue and Karthik Enamundram, Naga and Baillet, Sylvain and Dudley, Roy and Cohen-Adad, Julien},
      journal={QBIN Scientific Day, Best Flash Talk},
      year={2022},
      html={https://event.fourwaves.com/fr/qbinscientificday2022/resumes/ad70d0ce-32ea-4a71-9e45-6ec34d772363},
      pdf={https://cdn.fourwaves.com/static/media/formdata/985f0c64-8ac6-4c6f-a03c-146a28691c26/a2be897c-4143-4f85-831d-192e205cece6.pdf},
      abstract={Epilepsy is the fourth most common neurological disorder in the world. It affects the central nervous system, leading to abnormal brain activity which causes seizures that are sometimes accompanied by loss of consciousness. Electroencephalography (EEG) and magnetoencephalography (MEG) recordings contain patterns of abnormal brain activity such as interictal epileptiform discharges (IEDs), also known as spikes, that aid in the diagnosis of epilepsy and the identification of the epileptogenic zone. Due to the long recording time, high number of channels, and their noisy nature such recordings are tedious and time consuming to manually analyze. These challenges have motivated the development of automated methods to detect epileptic spikes. Convolutional and Recurrent Neural Networks are among the most frequently chosen architectures for their feature extraction capacities. However, both methods have limited global dependencies perception and recurrent networks lack efficiency because the steps cannot be parallelized. We developed a model based on a transformer architecture. Transformers are a breakthrough in the fields of Natural Language Processing and Computer Vision. They rely on a self-attention mechanism, which enables to differentially weight the significance of each part of the input data enhancing relevant ones while diminishing others. The aim of our framework is to detect spikes on EEG and/or MEG signals while being agnostic to the number of channels. First, the raw data is preprocessed (artifact removal, notch filter, etc.) and split into 2-second segments called ‘trials’ using the open-source Brainstorm software (https://neuroimage.usc.edu/brainstorm/). Spatial filtering is performed before applying the attention mechanism on the feature-channel dimension to focus on relevant channels. Then, embeddings of each time point are created before entering a transformer encoder to perceive global temporal dependencies. The output of the encoder is a highly distinguishable representation of the trial containing spatial and temporal information. The last block is composed of two fully-connected layers separated by a Mish activation function. It splits the data into 10 time windows and gives the probability of presence of a spike for each time window. Preliminary experiments were conducted on a single pediatric participant. A repeated 5-fold cross-validation strategy was used for training and validation. The first experimental results are promising with an accuracy of 90% and a F1-score of 70%. Our model has good potential for spatial and temporal features learning on EEG and MEG signals. The next step is to perform cross-subject spike detection to have a robust framework usable in real-world situations.},
      selected = {false},
}



